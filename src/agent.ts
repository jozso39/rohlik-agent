// agent.ts - Main LangGraph agent with MCP tools integration

import { ChatOpenAI } from "@langchain/openai";
import { AIMessage, SystemMessage } from "@langchain/core/messages";
import { ToolNode } from "@langchain/langgraph/prebuilt";
import { MessagesAnnotation, StateGraph } from "@langchain/langgraph";
import { mcpTools } from "./tools/mcpTools.ts";

// Define the tools for the agent to use - only MCP tools for recipe focus
const tools = [...mcpTools];
const toolNode = new ToolNode(tools);

const model = new ChatOpenAI({
    model: "gpt-4o-mini",
    temperature: 0,
    streaming: true, // Enable streaming
}).bindTools(tools);

/**
 * Determines the next action based on the last message in the conversation.
 *
 * If the last message contains tool calls, returns "tools" to indicate that tool processing should continue.
 * Otherwise, returns "__end__" to signal the end of the process.
 *
 * @param {MessagesAnnotation.State} param0 - An object containing the array of messages.
 * @returns {"tools" | "__end__"} - The next action to take.
 */
function shouldContinue({ messages }: typeof MessagesAnnotation.State) {
    const lastMessage = messages[messages.length - 1] as AIMessage;

    if (lastMessage.tool_calls?.length) {
        return "tools";
    }
    return "__end__";
}

const systemMessageText =
    "Jsi uÅ¾iteÄnÃ½ asistent, kterÃ½ komunikuje s uÅ¾ivateli VÃHRADNÄš V ÄŒEÅ TINÄš!" +
    "RadÃ­Å¡ uÅ¾ivatelÅ¯m s recepty a jsi schopnÃ½ tÄ›chto ÃºkonÅ¯:" +
    "- pÅ™idÃ¡vat a odebÃ­rat ingredience z nÃ¡kupnÃ­ho seznamu" +
    "- vyhledÃ¡vat recepty podle diety nebo typu jÃ­dla pomocÃ­ MCP serveru" +
    "- plÃ¡novat jÃ­delnÃ­Äek na vÃ­ce dnÃ­ podle dietnÃ­ch poÅ¾adavkÅ¯ uÅ¾ivatele" +
    "- vytvÃ¡Å™et dokument s jÃ­delnÃ­Äkem" +
    "\n\nPro vyhledÃ¡vÃ¡nÃ­ receptÅ¯ pouÅ¾Ã­vej nÃ¡stroje search_recipes a get_all_recipes." +
    "Pokud nenajdeÅ¡ recepty pro specifickou dietu, navrhni alternativy z dostupnÃ½ch receptÅ¯." +
    "\n\nKdyÅ¾ vytvÃ¡Å™Ã­Å¡ jÃ­delnÃ­Äek, VÅ½DY ho prezentuj v tomto formÃ¡tu:" +
    "\nğŸ“… JÃDELNÃÄŒEK:" +
    "\nğŸ—“ï¸ Den 1 (pondÄ›lÃ­):" +
    "\n  â€¢ SnÃ­danÄ›: [nÃ¡zev receptu]" +
    "\n  â€¢ ObÄ›d: [nÃ¡zev receptu]" +
    "\n  â€¢ VeÄeÅ™e: [nÃ¡zev receptu]" +
    "\nğŸ—“ï¸ Den 2 (ÃºterÃ½):" +
    "\n  â€¢ SnÃ­danÄ›: [nÃ¡zev receptu]" +
    "\n  â€¢ atd..." +
    "\n\nVÅ¾dy pÅ™idej vÅ¡echny ingredience z vybranÃ½ch receptÅ¯ na nÃ¡kupnÃ­ seznam. " +
    "\nPokud si uÅ¾ivatel vyÅ¾Ã¡dÃ¡ vytvoÅ™enÃ­ jÃ­delnÃ­Äku nebo plÃ¡nu jÃ­del na vÃ­ce dnÃ­, rovnou vytvoÅ™ i markdown dokument s tÃ­mto jÃ­delnÃ­Äkem";
"\nVÅ¡e na co odpovÃ­dÃ¡Å¡ se pÃ­Å¡e do bash konzole, formÃ¡tuj odpovÄ›di podle toho (nepouÅ¾Ã­vej markdown formÃ¡tovÃ¡nÃ­)";

/**
 * Invokes the model with the current state messages, prepending a system message.
 *
 * @param state - The current state containing messages to be processed.
 * @returns A promise that resolves to an object containing the model's response message.
 */
async function callModel(state: typeof MessagesAnnotation.State) {
    const systemMessage = new SystemMessage(systemMessageText);
    const messagesWithSystem = [systemMessage, ...state.messages];
    const response = await model.invoke(messagesWithSystem);
    return { messages: [response] };
}

const workflow = new StateGraph(MessagesAnnotation)
    .addNode("agent", callModel)
    .addEdge("__start__", "agent")
    .addNode("tools", toolNode)
    .addEdge("tools", "agent")
    .addConditionalEdges("agent", shouldContinue);

const app = workflow.compile();

export { app };
